{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c201e9-9588-4ab8-a2ad-0df0fe8acf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: 404, {\n",
      "  \"error\": {\n",
      "    \"type\": \"record-not-found\",\n",
      "    \"message\": \"Actor with this name was not found\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     exit()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 7Ô∏è‚É£ Convert the data into a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tweets)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 8Ô∏è‚É£ Select relevant columns (modify if needed)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Your Apify API Token (Get it from https://console.apify.com/)\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 2Ô∏è‚É£ Twitter search query (e.g., \"Solana meme coin\")\n",
    "query = \"Solana meme coin\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API Endpoint for Twitter Scraper\n",
    "url = f\"https://api.apify.com/v2/acts/misceris~twitter-scraper/run-sync-get-dataset-items?token={apify_token}\"\n",
    "\n",
    "# 4Ô∏è‚É£ Set up the request payload\n",
    "payload = {\n",
    "    \"queries\": [query],  \n",
    "    \"mode\": \"live\",  # Fetch recent tweets\n",
    "    \"resultsLimit\": 100  # Adjust as needed (100, 500, etc.)\n",
    "}\n",
    "\n",
    "# 5Ô∏è‚É£ Send request to Apify\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# 6Ô∏è‚É£ Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data extracted successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 7Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 8Ô∏è‚É£ Select relevant columns (modify if needed)\n",
    "if not df.empty:\n",
    "    df = df[['id', 'username', 'text', 'timestamp', 'likes', 'retweets']]\n",
    "\n",
    "# 9Ô∏è‚É£ Save the DataFrame to an Excel file\n",
    "file_name = \"solana_tweets.xlsx\"\n",
    "df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Data saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285b3461-40a9-45fc-8ff9-2d71719b153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved successfully!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['username', 'timestamp', 'likes', 'retweets'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Select relevant columns (modify if needed)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 29\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretweets\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Save to Excel\u001b[39;00m\n\u001b[0;32m     32\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolana_tweets.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['username', 'timestamp', 'likes', 'retweets'] not in index\""
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Your Apify API token\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# Your dataset ID (from Apify dashboard)\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# Apify API URL to get dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/VNyYxPeAmtemPjQiH/items?token=apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# Convert data to Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# Select relevant columns (modify if needed)\n",
    "if not df.empty:\n",
    "    df = df[['id', 'username', 'text', 'timestamp', 'likes', 'retweets']]\n",
    "\n",
    "# Save to Excel\n",
    "file_name = \"solana_tweets.xlsx\"\n",
    "df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Data saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c774c8c5-b620-44ca-86a0-74cc2f688683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved successfully!\n",
      "üîç Available Columns: Index(['noResults', 'type', 'id', 'url', 'twitterUrl', 'text', 'fullText',\n",
      "       'source', 'retweetCount', 'replyCount', 'likeCount', 'quoteCount',\n",
      "       'viewCount', 'createdAt', 'lang', 'bookmarkCount', 'isReply',\n",
      "       'conversationId', 'isPinned', 'author', 'extendedEntities', 'card',\n",
      "       'place', 'entities', 'isRetweet', 'retweet', 'isQuote', 'media',\n",
      "       'isConversationControlled', 'quoteId', 'quote'],\n",
      "      dtype='object')\n",
      "‚úÖ Data saved to solana_tweets.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 1Ô∏è‚É£ Apify API Token\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 2Ô∏è‚É£ Dataset ID from Apify Storage\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API URL to fetch dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/VNyYxPeAmtemPjQiH/items?token=apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 4Ô∏è‚É£ Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# 5Ô∏è‚É£ Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 6Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 7Ô∏è‚É£ Check if the dataset is empty\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No tweets found in the dataset!\")\n",
    "    exit()\n",
    "\n",
    "# 8Ô∏è‚É£ Print actual columns in dataset\n",
    "print(\"üîç Available Columns:\", df.columns)\n",
    "\n",
    "# 9Ô∏è‚É£ Select only available columns from your dataset\n",
    "available_columns = ['url', 'twitterUrl', 'id', 'text', 'retweetCount', 'replyCount']\n",
    "df = df[[col for col in available_columns if col in df.columns]]\n",
    "\n",
    "# üîü Save the DataFrame to Excel\n",
    "file_name = \"solana_tweets.xlsx\"\n",
    "df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Data saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a6579a9-5506-4b88-a6ad-52df0689e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved successfully!\n",
      "üîç Available Columns: Index(['noResults', 'type', 'id', 'url', 'twitterUrl', 'text', 'fullText',\n",
      "       'source', 'retweetCount', 'replyCount', 'likeCount', 'quoteCount',\n",
      "       'viewCount', 'createdAt', 'lang', 'bookmarkCount', 'isReply',\n",
      "       'conversationId', 'isPinned', 'author', 'extendedEntities', 'card',\n",
      "       'place', 'entities', 'isRetweet', 'retweet', 'isQuote', 'media',\n",
      "       'isConversationControlled', 'quoteId', 'quote'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_coins\u001b[39m(text):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[A-Z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Finds \"$COIN\" or \"TOKEN\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoins\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_coins(x))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 1Ô∏è‚É£1Ô∏è‚É£ Flatten list of all coin mentions\u001b[39;00m\n\u001b[0;32m     48\u001b[0m all_coins \u001b[38;5;241m=\u001b[39m [coin\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoins\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m coin \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_coins\u001b[39m(text):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[A-Z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Finds \"$COIN\" or \"TOKEN\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoins\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: extract_coins(x))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 1Ô∏è‚É£1Ô∏è‚É£ Flatten list of all coin mentions\u001b[39;00m\n\u001b[0;32m     48\u001b[0m all_coins \u001b[38;5;241m=\u001b[39m [coin\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoins\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m coin \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36mextract_coins\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_coins\u001b[39m(text):\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb[A-Z]\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:217\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39mfindall(string)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1Ô∏è‚É£ Apify API Token\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 2Ô∏è‚É£ Dataset ID from Apify Storage\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API URL to fetch dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/VNyYxPeAmtemPjQiH/items?token=apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 4Ô∏è‚É£ Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# 5Ô∏è‚É£ Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 6Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 7Ô∏è‚É£ Check if dataset is empty\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No tweets found in the dataset!\")\n",
    "    exit()\n",
    "\n",
    "# 8Ô∏è‚É£ Print available columns for debugging\n",
    "print(\"üîç Available Columns:\", df.columns)\n",
    "\n",
    "# 9Ô∏è‚É£ Select only necessary columns\n",
    "available_columns = ['text', 'retweetCount', 'replyCount']\n",
    "df = df[[col for col in available_columns if col in df.columns]]\n",
    "\n",
    "# üîü Extract potential coin names (words starting with \"$\" or all caps)\n",
    "def extract_coins(text):\n",
    "    return re.findall(r'\\$\\w+|\\b[A-Z]{2,}\\b', text)  # Finds \"$COIN\" or \"TOKEN\"\n",
    "\n",
    "df[\"coins\"] = df[\"text\"].apply(lambda x: extract_coins(x))\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Flatten list of all coin mentions\n",
    "all_coins = [coin.upper() for sublist in df[\"coins\"] for coin in sublist]\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Count occurrences of each coin/token\n",
    "coin_counts = Counter(all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Convert to DataFrame\n",
    "coins_df = pd.DataFrame(coin_counts.items(), columns=[\"Coin\", \"Mentions\"])\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Calculate total engagement (retweets + replies) for each token\n",
    "df[\"total_engagement\"] = df[\"retweetCount\"] + df[\"replyCount\"]\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Aggregate engagement per coin\n",
    "coin_engagement = df.explode(\"coins\").groupby(\"coins\")[\"total_engagement\"].sum().reset_index()\n",
    "coin_engagement.columns = [\"Coin\", \"Total Engagement\"]\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ Merge mentions & engagement data\n",
    "top_coins_df = coins_df.merge(coin_engagement, on=\"Coin\", how=\"left\").fillna(0)\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ Sort by mentions & engagement\n",
    "top_coins_df = top_coins_df.sort_values(by=[\"Mentions\", \"Total Engagement\"], ascending=False)\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ Save the results to Excel\n",
    "file_name = \"top_trending_solana_coins.xlsx\"\n",
    "top_coins_df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Top trending Solana meme coins saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0acbfaf4-5092-4b00-afb0-c303c2b62dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved successfully!\n",
      "üîç Available Columns: Index(['noResults', 'type', 'id', 'url', 'twitterUrl', 'text', 'fullText',\n",
      "       'source', 'retweetCount', 'replyCount', 'likeCount', 'quoteCount',\n",
      "       'viewCount', 'createdAt', 'lang', 'bookmarkCount', 'isReply',\n",
      "       'conversationId', 'isPinned', 'author', 'extendedEntities', 'card',\n",
      "       'place', 'entities', 'isRetweet', 'retweet', 'isQuote', 'media',\n",
      "       'isConversationControlled', 'quoteId', 'quote'],\n",
      "      dtype='object')\n",
      "‚úÖ Top trending Solana meme coins saved to top_trending_solana_coins.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1Ô∏è‚É£ Apify API Token\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 2Ô∏è‚É£ Dataset ID from Apify Storage\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API URL to fetch dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/VNyYxPeAmtemPjQiH/items?token=apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 4Ô∏è‚É£ Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# 5Ô∏è‚É£ Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 6Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 7Ô∏è‚É£ Check if dataset is empty\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No tweets found in the dataset!\")\n",
    "    exit()\n",
    "\n",
    "# 8Ô∏è‚É£ Print available columns for debugging\n",
    "print(\"üîç Available Columns:\", df.columns)\n",
    "\n",
    "# 9Ô∏è‚É£ Select only necessary columns\n",
    "available_columns = ['text', 'retweetCount', 'replyCount']\n",
    "df = df[[col for col in available_columns if col in df.columns]]\n",
    "\n",
    "# üîü Convert 'text' column to string and handle NaN values\n",
    "df[\"text\"] = df[\"text\"].astype(str).fillna(\"\")\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Extract potential coin names (words starting with \"$\" or all caps)\n",
    "def extract_coins(text):\n",
    "    return re.findall(r'\\$\\w+|\\b[A-Z]{2,}\\b', text)  # Finds \"$COIN\" or \"TOKEN\"\n",
    "\n",
    "df[\"coins\"] = df[\"text\"].apply(extract_coins)\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Flatten list of all coin mentions\n",
    "all_coins = [coin.upper() for sublist in df[\"coins\"] for coin in sublist]\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Count occurrences of each coin/token\n",
    "coin_counts = Counter(all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Convert to DataFrame\n",
    "coins_df = pd.DataFrame(coin_counts.items(), columns=[\"Coin\", \"Mentions\"])\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Calculate total engagement (retweets + replies) for each token\n",
    "df[\"total_engagement\"] = df[\"retweetCount\"] + df[\"replyCount\"]\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ Aggregate engagement per coin\n",
    "coin_engagement = df.explode(\"coins\").groupby(\"coins\")[\"total_engagement\"].sum().reset_index()\n",
    "coin_engagement.columns = [\"Coin\", \"Total Engagement\"]\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ Merge mentions & engagement data\n",
    "top_coins_df = coins_df.merge(coin_engagement, on=\"Coin\", how=\"left\").fillna(0)\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ Sort by mentions & engagement\n",
    "top_coins_df = top_coins_df.sort_values(by=[\"Mentions\", \"Total Engagement\"], ascending=False)\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ Save the results to Excel\n",
    "file_name = \"top_trending_solana_coins.xlsx\"\n",
    "top_coins_df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Top trending Solana meme coins saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f55dda-ea02-41d8-825a-dccf0d61f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved successfully!\n",
      "üîç Available Columns: Index(['noResults', 'type', 'id', 'url', 'twitterUrl', 'text', 'fullText',\n",
      "       'source', 'retweetCount', 'replyCount', 'likeCount', 'quoteCount',\n",
      "       'viewCount', 'createdAt', 'lang', 'bookmarkCount', 'isReply',\n",
      "       'conversationId', 'isPinned', 'author', 'extendedEntities', 'card',\n",
      "       'place', 'entities', 'isRetweet', 'retweet', 'isQuote', 'media',\n",
      "       'isConversationControlled', 'quoteId', 'quote'],\n",
      "      dtype='object')\n",
      "‚úÖ Top trending Solana meme coins saved to top_trending_solana_meme_coins.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1Ô∏è‚É£ Apify API Token\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 2Ô∏è‚É£ Dataset ID from Apify Storage\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API URL to fetch dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/VNyYxPeAmtemPjQiH/items?token=apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 4Ô∏è‚É£ Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# 5Ô∏è‚É£ Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 6Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 7Ô∏è‚É£ Check if dataset is empty\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No tweets found in the dataset!\")\n",
    "    exit()\n",
    "\n",
    "# 8Ô∏è‚É£ Print available columns for debugging\n",
    "print(\"üîç Available Columns:\", df.columns)\n",
    "\n",
    "# 9Ô∏è‚É£ Select only necessary columns\n",
    "available_columns = ['text', 'retweetCount', 'replyCount']\n",
    "df = df[[col for col in available_columns if col in df.columns]]\n",
    "\n",
    "# üîü Convert 'text' column to string and handle NaN values\n",
    "df[\"text\"] = df[\"text\"].astype(str).fillna(\"\")\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **List of known Solana meme coins**\n",
    "solana_meme_coins = {\"BONK\", \"WIF\", \"SAMO\", \"POP\", \"HNT\", \"KIN\", \"COPE\", \"TULIP\", \"DUST\", \"SHDW\"}\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Extract potential coin names (words starting with \"$\" or matching known Solana coins)\n",
    "def extract_coins(text):\n",
    "    matches = re.findall(r'\\$\\w+', text)  # Find words starting with \"$\"\n",
    "    words = set(re.findall(r'\\b[A-Z]{2,}\\b', text))  # Find all uppercase words\n",
    "    valid_coins = words.intersection(solana_meme_coins)  # Only keep known meme coins\n",
    "    return matches + list(valid_coins)\n",
    "\n",
    "df[\"coins\"] = df[\"text\"].apply(lambda x: extract_coins(x))\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Flatten list of all coin mentions\n",
    "all_coins = [coin.upper() for sublist in df[\"coins\"] for coin in sublist]\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Count occurrences of each coin/token\n",
    "coin_counts = Counter(all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Convert to DataFrame\n",
    "coins_df = pd.DataFrame(coin_counts.items(), columns=[\"Coin\", \"Mentions\"])\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ Calculate total engagement (retweets + replies) for each token\n",
    "df[\"total_engagement\"] = df[\"retweetCount\"] + df[\"replyCount\"]\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ Aggregate engagement per coin\n",
    "coin_engagement = df.explode(\"coins\").groupby(\"coins\")[\"total_engagement\"].sum().reset_index()\n",
    "coin_engagement.columns = [\"Coin\", \"Total Engagement\"]\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ Merge mentions & engagement data\n",
    "top_coins_df = coins_df.merge(coin_engagement, on=\"Coin\", how=\"left\").fillna(0)\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ Sort by mentions & engagement\n",
    "top_coins_df = top_coins_df.sort_values(by=[\"Mentions\", \"Total Engagement\"], ascending=False)\n",
    "\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£ Save the results to Excel\n",
    "file_name = \"top_trending_solana_meme_coins.xlsx\"\n",
    "top_coins_df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Top trending Solana meme coins saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20006e1-4faa-40de-884d-2de84d8af140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved successfully!\n",
      "üîç Available Columns: Index(['noResults', 'type', 'id', 'url', 'twitterUrl', 'text', 'fullText',\n",
      "       'source', 'retweetCount', 'replyCount', 'likeCount', 'quoteCount',\n",
      "       'viewCount', 'createdAt', 'lang', 'bookmarkCount', 'isReply',\n",
      "       'conversationId', 'isPinned', 'author', 'extendedEntities', 'card',\n",
      "       'place', 'entities', 'isRetweet', 'retweet', 'isQuote', 'media',\n",
      "       'isConversationControlled', 'quoteId', 'quote'],\n",
      "      dtype='object')\n",
      "üîç Extracted Coins: ['MUSK', 'UNIQUELY', 'ROBOT', 'HUMANOID', 'POSITIONED', 'MANUFACTURE', 'DOGE', 'EINSTEIN', 'AS', 'EACH', 'CHILD', 'HAVING', 'TEACHER']\n",
      "‚úÖ Data saved to top_trending_twitter_solana_coins.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1Ô∏è‚É£ Apify API Token\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 2Ô∏è‚É£ Dataset ID from Apify Storage\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API URL to fetch dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/VNyYxPeAmtemPjQiH/items?token=apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 4Ô∏è‚É£ Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# 5Ô∏è‚É£ Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 6Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 7Ô∏è‚É£ Check if dataset is empty\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No tweets found in the dataset! Try increasing tweet limit in Apify.\")\n",
    "    exit()\n",
    "\n",
    "# 8Ô∏è‚É£ Print available columns for debugging\n",
    "print(\"üîç Available Columns:\", df.columns)\n",
    "\n",
    "# 9Ô∏è‚É£ Select only necessary columns\n",
    "available_columns = ['text', 'retweetCount', 'replyCount']\n",
    "df = df[[col for col in available_columns if col in df.columns]]\n",
    "\n",
    "# üîü Convert 'text' column to string and handle NaN values\n",
    "df[\"text\"] = df[\"text\"].astype(str).fillna(\"\")\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **List of known Solana meme coins (optional)**\n",
    "solana_meme_coins = {\"BONK\", \"WIF\", \"SAMO\", \"POP\", \"HNT\", \"KIN\", \"COPE\", \"TULIP\", \"DUST\", \"SHDW\"}\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Extract potential coin names (words starting with \"$\" or uppercase tokens)\n",
    "def extract_coins(text):\n",
    "    matches = re.findall(r'\\$\\w+', text)  # Find \"$TOKEN\" mentions\n",
    "    words = set(re.findall(r'\\b[A-Z]{2,}\\b', text))  # Find all uppercase words\n",
    "\n",
    "    # Exclude common words that are not crypto-related\n",
    "    exclude_words = {\"RT\", \"ELON\", \"TESLA\", \"AI\", \"EDUCATION\", \"IS\", \"TO\", \"THE\", \"FOR\", \"WILL\", \"BE\", \"LIKE\"}\n",
    "    valid_coins = [word for word in words if word not in exclude_words]\n",
    "\n",
    "    # Only keep words that are in Solana meme coin list (if using the strict filter)\n",
    "    detected_coins = matches + valid_coins  # More flexible detection\n",
    "    return detected_coins\n",
    "\n",
    "df[\"coins\"] = df[\"text\"].apply(lambda x: extract_coins(str(x)))\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Flatten list of all coin mentions\n",
    "all_coins = [coin.upper() for sublist in df[\"coins\"] for coin in sublist]\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Print extracted coins to debug\n",
    "print(\"üîç Extracted Coins:\", all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Count occurrences of each coin/token\n",
    "coin_counts = Counter(all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ Convert to DataFrame\n",
    "coins_df = pd.DataFrame(coin_counts.items(), columns=[\"Coin\", \"Mentions\"])\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ Calculate total engagement (retweets + replies) for each token\n",
    "df[\"total_engagement\"] = df[\"retweetCount\"] + df[\"replyCount\"]\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ Aggregate engagement per coin\n",
    "coin_engagement = df.explode(\"coins\").groupby(\"coins\")[\"total_engagement\"].sum().reset_index()\n",
    "coin_engagement.columns = [\"Coin\", \"Total Engagement\"]\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ Merge mentions & engagement data\n",
    "top_coins_df = coins_df.merge(coin_engagement, on=\"Coin\", how=\"left\").fillna(0)\n",
    "\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£ Sort by mentions & engagement\n",
    "top_coins_df = top_coins_df.sort_values(by=[\"Mentions\", \"Total Engagement\"], ascending=False)\n",
    "\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£ **Check if there are extracted meme coins before saving**\n",
    "if not top_coins_df.empty:\n",
    "    file_name = \"top_trending_twitter_solana_coins.xlsx\"\n",
    "    top_coins_df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "    print(f\"‚úÖ Data saved to {file_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No meme coins detected! Try increasing tweet limit or adjusting filters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc77eabc-99c9-43a7-940e-e608ae25ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved successfully!\n",
      "üîç Available Columns: Index(['noResults', 'type', 'id', 'url', 'twitterUrl', 'text', 'fullText',\n",
      "       'source', 'retweetCount', 'replyCount', 'likeCount', 'quoteCount',\n",
      "       'viewCount', 'createdAt', 'lang', 'bookmarkCount', 'isReply',\n",
      "       'conversationId', 'isPinned', 'author', 'extendedEntities', 'card',\n",
      "       'place', 'entities', 'isRetweet', 'retweet', 'isQuote', 'media',\n",
      "       'isConversationControlled', 'quoteId', 'quote'],\n",
      "      dtype='object')\n",
      "üîç Extracted Coins: ['DOGE']\n",
      "‚úÖ Data saved to top_twitter_solana_coins.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1Ô∏è‚É£ Apify API Token\n",
    "apify_token = \"apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 2Ô∏è‚É£ Dataset ID from Apify Storage\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API URL to fetch dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/VNyYxPeAmtemPjQiH/items?token=apify_api_8w5fwlmKxmlV8fq8y2LWX1Pp6hLrST0ISAHR\"\n",
    "\n",
    "# 4Ô∏è‚É£ Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# 5Ô∏è‚É£ Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 6Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 7Ô∏è‚É£ Check if dataset is empty\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No tweets found in the dataset!\")\n",
    "    exit()\n",
    "\n",
    "# 8Ô∏è‚É£ Print available columns for debugging\n",
    "print(\"üîç Available Columns:\", df.columns)\n",
    "\n",
    "# 9Ô∏è‚É£ Select only necessary columns\n",
    "available_columns = ['text', 'retweetCount', 'replyCount']\n",
    "df = df[[col for col in available_columns if col in df.columns]]\n",
    "\n",
    "# üîü Convert 'text' column to string and handle NaN values\n",
    "df[\"text\"] = df[\"text\"].astype(str).fillna(\"\")\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **List of known Solana meme coins** (can be expanded as needed)\n",
    "solana_meme_coins = {\"BONK\", \"WIF\", \"SAMO\", \"POP\", \"HNT\", \"KIN\", \"COPE\", \"TULIP\", \"DUST\", \"SHDW\", \"DOGE\"}\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Extract potential coin names (words starting with \"$\" or matching known Solana coins)\n",
    "def extract_coins(text):\n",
    "    matches = re.findall(r'\\$\\w+', text)  # Find \"$TOKEN\" mentions\n",
    "    words = set(re.findall(r'\\b[A-Z]{2,}\\b', text))  # Find all uppercase words\n",
    "\n",
    "    # Exclude common words that are not crypto-related\n",
    "    exclude_words = {\"RT\", \"ELON\", \"TESLA\", \"AI\", \"EDUCATION\", \"IS\", \"TO\", \"THE\", \"FOR\", \"WILL\", \"BE\", \"LIKE\", \"EINSTEIN\", \"MUSK\"}\n",
    "    valid_coins = [word for word in words if word not in exclude_words]\n",
    "\n",
    "    # Only keep words that are in Solana meme coin list\n",
    "    detected_coins = matches + [word for word in valid_coins if word in solana_meme_coins]\n",
    "    return detected_coins\n",
    "\n",
    "df[\"coins\"] = df[\"text\"].apply(lambda x: extract_coins(str(x)))\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Flatten list of all coin mentions\n",
    "all_coins = [coin.upper() for sublist in df[\"coins\"] for coin in sublist]\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Print extracted coins to debug\n",
    "print(\"üîç Extracted Coins:\", all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Count occurrences of each coin/token\n",
    "coin_counts = Counter(all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ Convert to DataFrame\n",
    "coins_df = pd.DataFrame(coin_counts.items(), columns=[\"Coin\", \"Mentions\"])\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ Calculate total engagement (retweets + replies) for each token\n",
    "df[\"total_engagement\"] = df[\"retweetCount\"] + df[\"replyCount\"]\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ Aggregate engagement per coin\n",
    "coin_engagement = df.explode(\"coins\").groupby(\"coins\")[\"total_engagement\"].sum().reset_index()\n",
    "coin_engagement.columns = [\"Coin\", \"Total Engagement\"]\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ Merge mentions & engagement data\n",
    "top_coins_df = coins_df.merge(coin_engagement, on=\"Coin\", how=\"left\").fillna(0)\n",
    "\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£ Sort by mentions & engagement\n",
    "top_coins_df = top_coins_df.sort_values(by=[\"Mentions\", \"Total Engagement\"], ascending=False)\n",
    "\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£ **Check if there are extracted meme coins before saving**\n",
    "if not top_coins_df.empty:\n",
    "    file_name = \"top_twitter_solana_coins.xlsx\"\n",
    "    top_coins_df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "    print(f\"‚úÖ Data saved to {file_name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No meme coins detected! Try increasing tweet limit or adjusting filters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66de53cb-984a-479c-b81e-3a1fbeeefbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: 401, {\n",
      "  \"error\": {\n",
      "    \"type\": \"user-or-token-not-found\",\n",
      "    \"message\": \"User was not found or authentication token is not valid\"\n",
      "  }\n",
      "}\n",
      "üîç Available Columns: Index(['noResults', 'type', 'id', 'url', 'twitterUrl', 'text', 'fullText',\n",
      "       'source', 'retweetCount', 'replyCount', 'likeCount', 'quoteCount',\n",
      "       'viewCount', 'createdAt', 'lang', 'bookmarkCount', 'isReply',\n",
      "       'conversationId', 'isPinned', 'author', 'extendedEntities', 'card',\n",
      "       'place', 'entities', 'isRetweet', 'retweet', 'isQuote', 'media',\n",
      "       'isConversationControlled', 'quoteId', 'quote'],\n",
      "      dtype='object')\n",
      "üîç Extracted Coins: ['DOGE']\n",
      "\n",
      "üìä **Top Solana Meme Coins on Twitter** üìä\n",
      "\n",
      "+---+------+----------+------------------+\n",
      "|   | Coin | Mentions | Total Engagement |\n",
      "+---+------+----------+------------------+\n",
      "| 0 | DOGE |    1     |      9037.0      |\n",
      "+---+------+----------+------------------+\n",
      "\n",
      "‚úÖ Data saved to top_twitter_solana_coins.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tabulate import tabulate  # For displaying table in console\n",
    "\n",
    "# 1Ô∏è‚É£ Apify API Token (Replace with your actual token)\n",
    "apify_token = \"your_apify_api_token_here\"\n",
    "\n",
    "# 2Ô∏è‚É£ Dataset ID from Apify Storage\n",
    "dataset_id = \"VNyYxPeAmtemPjQiH\"\n",
    "\n",
    "# 3Ô∏è‚É£ Apify API URL to fetch dataset items\n",
    "url = f\"https://api.apify.com/v2/datasets/{dataset_id}/items?token={apify_token}\"\n",
    "\n",
    "# 4Ô∏è‚É£ Send request to Apify\n",
    "response = requests.get(url)\n",
    "\n",
    "# 5Ô∏è‚É£ Check if request was successful\n",
    "if response.status_code == 200:\n",
    "    tweets = response.json()\n",
    "    print(\"‚úÖ Data retrieved successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}, {response.text}\")\n",
    "    exit()\n",
    "\n",
    "# 6Ô∏è‚É£ Convert the data into a Pandas DataFrame\n",
    "df = pd.DataFrame(tweets)\n",
    "\n",
    "# 7Ô∏è‚É£ Check if dataset is empty\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No tweets found in the dataset!\")\n",
    "    exit()\n",
    "\n",
    "# 8Ô∏è‚É£ Print available columns for debugging\n",
    "print(\"üîç Available Columns:\", df.columns)\n",
    "\n",
    "# 9Ô∏è‚É£ Select only necessary columns\n",
    "available_columns = ['text', 'retweetCount', 'replyCount']\n",
    "df = df[[col for col in available_columns if col in df.columns]]\n",
    "\n",
    "# üîü Convert 'text' column to string and handle NaN values\n",
    "df[\"text\"] = df[\"text\"].astype(str).fillna(\"\")\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ **List of known Solana meme coins** (can be expanded as needed)\n",
    "solana_meme_coins = {\"BONK\", \"WIF\", \"SAMO\", \"POP\", \"HNT\", \"KIN\", \"COPE\", \"TULIP\", \"DUST\", \"SHDW\", \"DOGE\"}\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Extract potential coin names (words starting with \"$\" or matching known Solana coins)\n",
    "def extract_coins(text):\n",
    "    matches = re.findall(r'\\$\\w+', text)  # Find \"$TOKEN\" mentions\n",
    "    words = set(re.findall(r'\\b[A-Z]{2,}\\b', text))  # Find all uppercase words\n",
    "\n",
    "    # Exclude common words that are not crypto-related\n",
    "    exclude_words = {\"RT\", \"ELON\", \"TESLA\", \"AI\", \"EDUCATION\", \"IS\", \"TO\", \"THE\", \"FOR\", \"WILL\", \"BE\", \"LIKE\", \"EINSTEIN\", \"MUSK\"}\n",
    "    valid_coins = [word for word in words if word not in exclude_words]\n",
    "\n",
    "    # Only keep words that are in Solana meme coin list\n",
    "    detected_coins = matches + [word for word in valid_coins if word in solana_meme_coins]\n",
    "    return detected_coins\n",
    "\n",
    "df[\"coins\"] = df[\"text\"].apply(lambda x: extract_coins(str(x)))\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Flatten list of all coin mentions\n",
    "all_coins = [coin.upper() for sublist in df[\"coins\"] for coin in sublist]\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Print extracted coins to debug\n",
    "print(\"üîç Extracted Coins:\", all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Count occurrences of each coin/token\n",
    "coin_counts = Counter(all_coins)\n",
    "\n",
    "# 1Ô∏è‚É£6Ô∏è‚É£ Convert to DataFrame\n",
    "coins_df = pd.DataFrame(coin_counts.items(), columns=[\"Coin\", \"Mentions\"])\n",
    "\n",
    "# 1Ô∏è‚É£7Ô∏è‚É£ Calculate total engagement (retweets + replies) for each token\n",
    "df[\"total_engagement\"] = df[\"retweetCount\"] + df[\"replyCount\"]\n",
    "\n",
    "# 1Ô∏è‚É£8Ô∏è‚É£ Aggregate engagement per coin\n",
    "coin_engagement = df.explode(\"coins\").groupby(\"coins\")[\"total_engagement\"].sum().reset_index()\n",
    "coin_engagement.columns = [\"Coin\", \"Total Engagement\"]\n",
    "\n",
    "# 1Ô∏è‚É£9Ô∏è‚É£ Merge mentions & engagement data\n",
    "top_coins_df = coins_df.merge(coin_engagement, on=\"Coin\", how=\"left\").fillna(0)\n",
    "\n",
    "# 2Ô∏è‚É£0Ô∏è‚É£ Sort by mentions & engagement\n",
    "top_coins_df = top_coins_df.sort_values(by=[\"Mentions\", \"Total Engagement\"], ascending=False)\n",
    "\n",
    "# 2Ô∏è‚É£1Ô∏è‚É£ Display as a table in the console\n",
    "if not top_coins_df.empty:\n",
    "    print(\"\\nüìä **Top Solana Meme Coins on Twitter** üìä\\n\")\n",
    "    print(tabulate(top_coins_df, headers=\"keys\", tablefmt=\"pretty\"))  # Pretty formatted table in console\n",
    "    \n",
    "    # Save to Excel\n",
    "    file_name = \"top_twitter_solana_coins.xlsx\"\n",
    "    top_coins_df.to_excel(file_name, index=False, engine='openpyxl')\n",
    "    print(f\"\\n‚úÖ Data saved to {file_name}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No meme coins detected! Try increasing tweet limit or adjusting filters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bc125-9cd8-4245-86f0-1d66b92404cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
